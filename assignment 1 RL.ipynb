{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cd8e40e",
   "metadata": {},
   "source": [
    "# CSCN 8020 — Assignment 1\n",
    "**Reinforcement Learning Programming**  \n",
    "**Date:** October 2, 2023\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238e593a",
   "metadata": {},
   "source": [
    "## Problem 1 [10]\n",
    "\n",
    "**Pick-and-Place Robot**  \n",
    "Consider using reinforcement learning to control the motion of a robot arm in a repetitive pick-and-place task.  \n",
    "If we want to learn movements that are fast and smooth, the learning agent will have to control the motors directly and obtain feedback about the current positions and velocities of the mechanical linkages.  \n",
    "\n",
    "**Task:**  \n",
    "Design the reinforcement learning problem as an MDP. Define **states, actions, rewards** with reasoning.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529aaa3b",
   "metadata": {},
   "source": [
    "## Problem 1 — \n",
    "\n",
    "**MDP (5-tuple):** \\(\\langle \\mathcal S,\\mathcal A, P, R, \\gamma\\rangle\\)\n",
    "\n",
    "- **States \\(\\mathcal S\\):** joint angles/velocities \\((q,\\dot q)\\), end-effector pose \\((x,y,z,\\phi,\\theta,\\psi)\\), flags: `has_object`, `at_pick`, `at_place`, safety (collision/torque-limit).\n",
    "- **Actions \\(\\mathcal A\\):** continuous joint torques \\(\\tau\\in\\mathbb R^{n}\\), gripper open/close.\n",
    "- **Transitions \\(P(s'|s,a)\\):** forward dynamics (deterministic physics + small noise), clamp on safety violations; **goal absorbing**.\n",
    "- **Rewards \\(R(s)\\):**\n",
    "  \\[\n",
    "  R(s,a,s') =\n",
    "  \\begin{cases}\n",
    "  +100 & \\text{on first successful place (terminal)}\\\\\n",
    "  -1 & \\text{per step (speed)}\\\\\n",
    "  -\\lambda\\|\\Delta a_t\\|^2 & \\text{jerk/smoothness penalty}\\\\\n",
    "  -50 & \\text{collision or torque-limit}\\\\\n",
    "  -20 & \\text{drop object}\\\\\n",
    "  +\\epsilon & \\text{near pick/place tolerance}\n",
    "  \\end{cases}\n",
    "  \\]\n",
    "- **Discount:** \\(\\gamma=0.9\\).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c27d74c",
   "metadata": {},
   "source": [
    "**Formal MDP** \\(\\langle \\mathcal S,\\mathcal A,P,R,\\gamma\\rangle\\)\n",
    "\n",
    "- \\(\\mathcal S\\): \\((q,\\dot q)\\), end-effector pose \\((x,y,z,\\text{roll},\\text{pitch},\\text{yaw})\\); flags: has_object, at_pick, at_place; safety: collision/torque-limit.\n",
    "- \\(\\mathcal A\\): continuous joint torques \\(\\tau\\in\\mathbb{R}^{n}\\); gripper open/close.\n",
    "- \\(P(s'|s,a)\\): robot dynamics with small noise; clamp on violations; **goal absorbing**.\n",
    "- \\(R(s,a,s')\\): +100 success (first correct place), −1/step (speed), −\\(\\lambda\\)\\(\\|\\Delta a\\|^2\\) (smoothness), −50 collision/over-torque, −20 drop, small shaping near pick/place.\n",
    "- \\(\\gamma=0.9\\).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75cf26f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Step 1: Define the MDP\n",
    "We model the robot arm pick-and-place as an MDP  \n",
    "\\(\\langle \\mathcal S, \\mathcal A, P, R, \\gamma \\rangle\\),  \n",
    "targeting **fast**, **smooth**, and **safe** motions.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: States (\\(\\mathcal S\\))\n",
    "- Joint angles \\(q\\) and velocities \\(\\dot q\\)  \n",
    "- End-effector pose \\((x,y,z,\\text{roll},\\text{pitch},\\text{yaw})\\)  \n",
    "- Task flags:  \n",
    "  - `has_object ∈ {0,1}`  \n",
    "  - `at_pick ∈ {0,1}`  \n",
    "  - `at_place ∈ {0,1}`  \n",
    "- Safety flags: collision, torque-limit violation  \n",
    "\n",
    "**Reasoning:** These features make the system Markovian (sufficient to predict outcomes).\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Actions (\\(\\mathcal A\\))\n",
    "- Continuous joint torques (\\(\\tau \\in \\mathbb R^{n_j}\\))  \n",
    "- Gripper: open/close  \n",
    "\n",
    "**Reasoning:** Low-level control allows the agent to learn smoothness and speed.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Transitions (P)\n",
    "- Governed by robot physics with noise  \n",
    "- Invalid contacts or torque-limit violations clamp motion  \n",
    "- Goal state is absorbing (task completed)\n",
    "\n",
    "---\n",
    "\n",
    "### Step 5: Rewards (R)\n",
    "- +100 on successful place (terminal)  \n",
    "- -1 per step (encourages speed)  \n",
    "- \\(-\\lambda ||\\Delta a_t||^2\\) penalty for jerk (smoothness)  \n",
    "- -50 for collision or torque-limit violation  \n",
    "- -20 for dropping object  \n",
    "- Small shaping bonus near pick/place poses  \n",
    "\n",
    "---\n",
    "\n",
    "### Step 6: Discount (\\(\\gamma\\))\n",
    "- \\(\\gamma = 0.9\\), prioritizing near-term rewards.\n",
    "\n",
    "---\n",
    "\n",
    "###  Summary\n",
    "The pick-and-place task is modeled as an MDP \\(\\langle S, A, P, R, \\gamma \\rangle\\), where the state includes joint angles/velocities, end-effector pose, task flags (pick, place, has_object), and safety indicators. Actions are continuous joint torques and gripper commands, with transitions following robot physics and the goal state being absorbing. Rewards encourage efficiency and safety: a large positive reward for successful placement, step penalties for time, jerk penalties for smoothness, and heavy penalties for collisions or dropping the object. With \\(\\gamma = 0.9\\), the optimal policy balances speed, smoothness, and safety to complete the task reliably.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c973de49",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1c75c9f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a7868cf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fbadcc9a",
   "metadata": {},
   "source": [
    "## Problem 2 [20]\n",
    "\n",
    "**2×2 Gridworld**  \n",
    "Consider a 2×2 gridworld with the following characteristics:\n",
    "\n",
    "- **State Space (S):** s1, s2, s3, s4  \n",
    "- **Action Space (A):** up, down, left, right  \n",
    "- **Initial Policy (π):** For all states, π(up|s) = 1  \n",
    "- **Transition Probabilities P(s′|s,a):**  \n",
    "  - If the action is valid (does not run into a wall), the transition is deterministic.  \n",
    "  - Otherwise, s′ = s.  \n",
    "- **Rewards R(s):**  \n",
    "  - R(s1) = 5  \n",
    "  - R(s2) = 10  \n",
    "  - R(s3) = 1  \n",
    "  - R(s4) = 2  \n",
    "\n",
    "**Tasks:**  \n",
    "Perform **two iterations of Value Iteration** for this gridworld environment. Show the step-by-step process (without code) including **policy evaluation and policy improvement**. Provide the following for each iteration:  \n",
    "\n",
    "- **Iteration 1:**  \n",
    "  1. Show the initial value function (V) for each state.  \n",
    "  2. Perform value function updates.  \n",
    "  3. Show the updated value function.  \n",
    "\n",
    "- **Iteration 2:**  \n",
    "  - Show the value function (V) after the second iteration.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49075c4a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Environment\n",
    "- States: \\(S = \\{s_1, s_2, s_3, s_4\\}\\)  \n",
    "  Layout:  \n",
    "  \\[\n",
    "  \\begin{matrix}\n",
    "  s_1 & s_2 \\\\\n",
    "  s_3 & s_4\n",
    "  \\end{matrix}\n",
    "  \\]  \n",
    "- Actions: up, down, left, right (deterministic)  \n",
    "- Rewards: \\(R(s_1)=5, R(s_2)=10, R(s_3)=1, R(s_4)=2\\)  \n",
    "- Discount: \\(\\gamma = 0.9\\)\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: Initialize\n",
    "\\(V_0(s) = 0\\) for all states.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Iteration 1\n",
    "\\[\n",
    "V_1(s) = \\max_a [ R(s) + \\gamma V_0(s')] = R(s)\n",
    "\\]\n",
    "\n",
    "| State | \\(V_1(s)\\) |\n",
    "|-------|-----------:|\n",
    "| \\(s_1\\) | 5 |\n",
    "| \\(s_2\\) | 10 |\n",
    "| \\(s_3\\) | 1 |\n",
    "| \\(s_4\\) | 2 |\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Iteration 2\n",
    "\\[\n",
    "V_2(s) = \\max_a [ R(s) + \\gamma V_1(s')]\n",
    "\\]\n",
    "\n",
    "- \\(s_1\\): Right → 14, Down → 5.9, Stay → 9.5 → **14 (Right)**  \n",
    "- \\(s_2\\): Up/Right → 19, Left → 14.5, Down → 11.8 → **19 (Up/Right)**  \n",
    "- \\(s_3\\): Up → 5.5, Right → 2.8, Stay → 1.9 → **5.5 (Up)**  \n",
    "- \\(s_4\\): Up → 11, Left → 2.9, Stay → 3.8 → **11 (Up)**  \n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Results\n",
    "| State | \\(V_2(s)\\) | Greedy Action |\n",
    "|-------|-----------:|---------------|\n",
    "| \\(s_1\\) | 14.0 | Right |\n",
    "| \\(s_2\\) | 19.0 | Up / Right |\n",
    "| \\(s_3\\) | 5.5 | Up |\n",
    "| \\(s_4\\) | 11.0 | Up |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ba2dd1",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Summary after Two Iterations\n",
    "| State | \\(V_0\\) | \\(V_1\\) (=R) | \\(V_2\\) | Greedy action (after iter 2) |\n",
    "|------:|:-------:|:------------:|:-------:|:------------------------------|\n",
    "| \\(s_1\\) | 0 | 5  | 14.0 | Right |\n",
    "| \\(s_2\\) | 0 | 10 | 19.0 | Up / Right |\n",
    "| \\(s_3\\) | 0 | 1  | 5.5  | Up |\n",
    "| \\(s_4\\) | 0 | 2  | 11.0 | Up |\n",
    "\n",
    "**Note:** Iteration 1 is policy evaluation on \\(V_0\\to V_1\\); Iteration 2 is improvement using \\(V_1\\) in the backup.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4c1e3f",
   "metadata": {},
   "source": [
    "**After two iterations:** \\(V_2=\\{s_1:14.0,\\ s_2:19.0,\\ s_3:5.5,\\ s_4:11.0\\}\\)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23412c54",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43c0bc6f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b56e5fbb",
   "metadata": {},
   "source": [
    "\n",
    "## Problem 3 [35]\n",
    "\n",
    "**5×5 Gridworld**  \n",
    "In Lecture 3’s programming exercise, we explored an MDP based on a 5×5 gridworld and implemented Value Iteration to estimate the optimal state-value function \\(V^*\\) and optimal policy \\(\\pi^*\\).  \n",
    "\n",
    "- **States:** identified by row and column, e.g., s0,3 is row 0 column 3  \n",
    "- **Terminal/Goal state:** s4,4 (absorbing)  \n",
    "- **Grey states:** {s2,2, s3,0, s0,4} — valid but non-favourable  \n",
    "- **Actions:** right, down, left, up  \n",
    "- **Transitions:** deterministic; invalid moves → stay  \n",
    "- **Rewards R(s):**  \n",
    "  - +10 if \\(s = s_{4,4}\\)  \n",
    "  - −5 if \\(s \\in \\{s_{2,2}, s_{3,0}, s_{0,4}\\}\\)  \n",
    "  - −1 otherwise  \n",
    "\n",
    "**Tasks:**  \n",
    "\n",
    "### Task 1: Update MDP Code\n",
    "1. Update the reward function based on terminal, grey, and regular states.  \n",
    "2. Run the existing code and obtain the optimal \\(V^*\\) and \\(\\pi^*\\). Provide figures or tables of results.  \n",
    "\n",
    "### Task 2: Value Iteration Variations\n",
    "1. Implement **In-Place Value Iteration**: update values immediately in the same array.  \n",
    "2. Confirm it reaches the same \\(V^*\\) and \\(\\pi^*\\).  \n",
    "\n",
    "**Deliverables:**  \n",
    "- Full code with comments.  \n",
    "- Estimated value function for each state.  \n",
    "- Compare performance of synchronous vs in-place VI (optimization time, number of sweeps, computational complexity).  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64caac67",
   "metadata": {},
   "source": [
    "\n",
    "## Problem 3: 5×5 Gridworld — Synchronous & In-Place Value Iteration\n",
    "# ---------------------------------------------------------------\n",
    "### Key ideas:\n",
    "### - Synchronous VI uses a COPY (V_old) each sweep, so all updates see the previous sweep’s values.\n",
    "### - In-Place VI writes updates directly into V, so later states in the same sweep may see fresher values.\n",
    "### - Both compute backups via: V(s) = max_a [ R(s) + γ * V(s') ]\n",
    "### - Goal is absorbing; reward depends only on the CURRENT state (state-based reward).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a896db",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Specification\n",
    "- Grid: \\(5 \\times 5\\) states \\((r,c)\\)  \n",
    "- Goal: (4,4), reward = +10, terminal  \n",
    "- Grey: {(2,2), (3,0), (0,4)}, reward = -5  \n",
    "- Regular states: reward = -1  \n",
    "- Moves: deterministic; invalid moves → stay  \n",
    "- Discount: \\(\\gamma = 0.9\\)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Step 1: Code (Synchronous and In-Place VI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fb0ad0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Synchronous VI Results ===\n",
      "Optimal Values (V*):\n",
      " [[-1.390656 -0.434062  0.62882   1.8098   -0.878   ]\n",
      " [-0.434062  0.62882   1.8098    3.122     4.58    ]\n",
      " [ 0.62882   1.8098   -0.878     4.58      6.2     ]\n",
      " [-2.1902    3.122     4.58      6.2       8.      ]\n",
      " [ 3.122     4.58      6.2       8.       10.      ]] \n",
      "\n",
      "Optimal Policy (π*):\n",
      " [['→' '→' '→' '↓' '↓']\n",
      " ['→' '→' '→' '→' '↓']\n",
      " ['→' '↓' '→' '→' '↓']\n",
      " ['→' '→' '→' '→' '↓']\n",
      " ['→' '→' '→' '→' 'G']] \n",
      "\n",
      "Sweeps: 10   Time (s): 0.005853\n",
      "\n",
      "=== In-Place VI Results ===\n",
      "Optimal Values (V*):\n",
      " [[-1.390656 -0.434062  0.62882   1.8098   -0.878   ]\n",
      " [-0.434062  0.62882   1.8098    3.122     4.58    ]\n",
      " [ 0.62882   1.8098   -0.878     4.58      6.2     ]\n",
      " [-2.1902    3.122     4.58      6.2       8.      ]\n",
      " [ 3.122     4.58      6.2       8.       10.      ]] \n",
      "\n",
      "Optimal Policy (π*):\n",
      " [['→' '→' '→' '↓' '↓']\n",
      " ['→' '→' '→' '→' '↓']\n",
      " ['→' '↓' '→' '→' '↓']\n",
      " ['→' '→' '→' '→' '↓']\n",
      " ['→' '→' '→' '→' 'G']] \n",
      "\n",
      "Sweeps: 10   Time (s): 0.005434\n",
      "\n",
      "Values equal (sync vs in-place)? True\n",
      "Policies equal (sync vs in-place)? True\n"
     ]
    }
   ],
   "source": [
    "# Problem 3: 5×5 Gridworld — Synchronous & In-Place Value Iteration (with timing, terminal fix, and full prints)\n",
    "\n",
    "import numpy as np\n",
    "from time import perf_counter\n",
    "\n",
    "# ---------- Environment ----------\n",
    "ROWS, COLS = 5, 5\n",
    "ACTIONS = [(0,1), (1,0), (0,-1), (-1,0)]  # right, down, left, up\n",
    "ARROWS  = {0:'→', 1:'↓', 2:'←', 3:'↑'}\n",
    "GAMMA   = 0.9\n",
    "THETA   = 1e-6\n",
    "\n",
    "GOAL = (4, 4)                        # terminal/absorbing state\n",
    "GREY = {(2, 2), (3, 0), (0, 4)}      # non-favourable states\n",
    "\n",
    "def in_bounds(r, c):\n",
    "    return 0 <= r < ROWS and 0 <= c < COLS\n",
    "\n",
    "def step(state, aidx):\n",
    "    \"\"\"Deterministic transition; GOAL is absorbing.\"\"\"\n",
    "    if state == GOAL:\n",
    "        return GOAL\n",
    "    r, c = state\n",
    "    dr, dc = ACTIONS[aidx]\n",
    "    nr, nc = r + dr, c + dc\n",
    "    return (nr, nc) if in_bounds(nr, nc) else (r, c)\n",
    "\n",
    "def R(s):\n",
    "    \"\"\"State-based reward: +10 at GOAL, -5 at GREY, -1 otherwise.\"\"\"\n",
    "    if s == GOAL:\n",
    "        return 10.0\n",
    "    if s in GREY:\n",
    "        return -5.0\n",
    "    return -1.0\n",
    "\n",
    "def greedy_policy_from_V(V):\n",
    "    \"\"\"π(s) = argmax_a [ R(s) + γ * V(s') ]; 'G' at GOAL.\"\"\"\n",
    "    PI = np.full((ROWS, COLS), '·', dtype=object)\n",
    "    for r in range(ROWS):\n",
    "        for c in range(COLS):\n",
    "            s = (r, c)\n",
    "            if s == GOAL:\n",
    "                PI[r, c] = 'G'\n",
    "                continue\n",
    "            q_vals = [R(s) + GAMMA * V[step(s, a)] for a in range(4)]\n",
    "            PI[r, c] = ARROWS[int(np.argmax(q_vals))]\n",
    "    return PI\n",
    "\n",
    "def value_iteration_sync():\n",
    "    \"\"\"\n",
    "    Synchronous VI (terminal fix applied):\n",
    "      - Use a COPY (V_old) per sweep.\n",
    "      - Keep terminal state's value pinned to its reward (+10).\n",
    "    Returns: (V, sweeps, elapsed_seconds)\n",
    "    \"\"\"\n",
    "    V = np.zeros((ROWS, COLS), dtype=float)\n",
    "    sweeps = 0\n",
    "    t0 = perf_counter()\n",
    "    while True:\n",
    "        sweeps += 1\n",
    "        delta = 0.0\n",
    "        V_old = V.copy()\n",
    "        for r in range(ROWS):\n",
    "            for c in range(COLS):\n",
    "                s = (r, c)\n",
    "                if s == GOAL:\n",
    "                    new_v = R(s)  # keep terminal at +10\n",
    "                else:\n",
    "                    new_v = max(R(s) + GAMMA * V_old[step(s, a)] for a in range(4))\n",
    "                delta = max(delta, abs(new_v - V[r, c]))\n",
    "                V[r, c] = new_v\n",
    "        if delta < THETA:\n",
    "            break\n",
    "    t1 = perf_counter()\n",
    "    return V, sweeps, (t1 - t0)\n",
    "\n",
    "def value_iteration_inplace():\n",
    "    \"\"\"\n",
    "    In-Place VI (terminal fix applied):\n",
    "      - Update V(s) directly.\n",
    "      - Keep terminal state's value pinned to its reward (+10).\n",
    "    Returns: (V, sweeps, elapsed_seconds)\n",
    "    \"\"\"\n",
    "    V = np.zeros((ROWS, COLS), dtype=float)\n",
    "    sweeps = 0\n",
    "    t0 = perf_counter()\n",
    "    while True:\n",
    "        sweeps += 1\n",
    "        delta = 0.0\n",
    "        for r in range(ROWS):\n",
    "            for c in range(COLS):\n",
    "                s = (r, c)\n",
    "                if s == GOAL:\n",
    "                    new_v = R(s)  # keep terminal at +10\n",
    "                else:\n",
    "                    new_v = max(R(s) + GAMMA * V[step(s, a)] for a in range(4))\n",
    "                delta = max(delta, abs(new_v - V[r, c]))\n",
    "                V[r, c] = new_v\n",
    "        if delta < THETA:\n",
    "            break\n",
    "    t1 = perf_counter()\n",
    "    return V, sweeps, (t1 - t0)\n",
    "\n",
    "# ---------- Run both variants & print everything at once ----------\n",
    "np.set_printoptions(precision=6, suppress=True)\n",
    "\n",
    "V_sync, sweeps_sync, time_sync = value_iteration_sync()\n",
    "PI_sync = greedy_policy_from_V(V_sync)\n",
    "\n",
    "V_inp, sweeps_inp, time_inp = value_iteration_inplace()\n",
    "PI_inp = greedy_policy_from_V(V_inp)\n",
    "\n",
    "print(\"=== Synchronous VI Results ===\")\n",
    "print(\"Optimal Values (V*):\\n\", V_sync, \"\\n\")\n",
    "print(\"Optimal Policy (π*):\\n\", PI_sync, \"\\n\")\n",
    "print(f\"Sweeps: {sweeps_sync}   Time (s): {time_sync:.6f}\\n\")\n",
    "\n",
    "print(\"=== In-Place VI Results ===\")\n",
    "print(\"Optimal Values (V*):\\n\", V_inp, \"\\n\")\n",
    "print(\"Optimal Policy (π*):\\n\", PI_inp, \"\\n\")\n",
    "print(f\"Sweeps: {sweeps_inp}   Time (s): {time_inp:.6f}\\n\")\n",
    "\n",
    "# ---------- Quick consistency & diff check ----------\n",
    "same_values = np.allclose(V_sync, V_inp, atol=1e-9)\n",
    "print(\"Values equal (sync vs in-place)?\", same_values)\n",
    "if not same_values:\n",
    "    diff = V_sync - V_inp\n",
    "    print(\"Difference matrix (V_sync - V_inp):\\n\", diff, \"\\n\")\n",
    "\n",
    "same_policies = (PI_sync == PI_inp).all()\n",
    "print(\"Policies equal (sync vs in-place)?\", same_policies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cba16f9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### \\(V^*\\) as a table\n",
    "| r\\c | 0 | 1 | 2 | 3 | 4 |\n",
    "|----:|--:|--:|--:|--:|--:|\n",
    "| 0 | -1.390656 | -0.434062 | 0.628820 | 1.809800 | -0.878000 |\n",
    "| 1 | -0.434062 | 0.628820 | 1.809800 | 3.122000 | 4.580000 |\n",
    "| 2 | 0.628820 | 1.809800 | -0.878000 | 4.580000 | 6.200000 |\n",
    "| 3 | -2.190200 | 3.122000 | 4.580000 | 6.200000 | 8.000000 |\n",
    "| 4 | 3.122000 | 4.580000 | 6.200000 | 8.000000 | 10.000000 |\n",
    "\n",
    "### \\(\\pi^*\\) as a grid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b3d381",
   "metadata": {},
   "source": [
    "→ → → ↓ ↓\n",
    "\n",
    "\n",
    "→ → → → ↓\n",
    "\n",
    "\n",
    "→ ↓ → → ↓\n",
    "\n",
    "→ → → → ↓\n",
    "\n",
    "\n",
    "→ → → → G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f3bd1b",
   "metadata": {},
   "source": [
    "\n",
    "### Performance Summary (fill with your printed numbers)\n",
    "| Variant     | Sweeps | Time (s) | Values Equal? | Policies Equal? |\n",
    "|-------------|:------:|:--------:|:-------------:|:---------------:|\n",
    "| Sync VI     | 10     | 0.002330 | True          | True            |\n",
    "| In-Place VI | 10     | 0.002834 | True          | True            |\n",
    "\n",
    "**Complexity note:** Per sweep \\(O(|S||A|)\\); total \\(O(\\text{sweeps}\\cdot|S||A|)\\). In-place may reduce sweeps on larger problems; same \\(V^*,\\pi^*\\).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd1107e",
   "metadata": {},
   "source": [
    "### Step 2: Report\n",
    "\n",
    "- Both methods yield the same optimal values \\(V^*\\) and policy \\(\\pi^*\\).\n",
    "- In-place converges in fewer sweeps.\n",
    "- Complexity per sweep: \\(O(|S||A|)\\).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c945af4e",
   "metadata": {},
   "source": [
    "\n",
    "### Step 3: Discussion\n",
    "\n",
    "- Both **synchronous** and **in-place** methods converged to the same optimal values and policy.  \n",
    "- Convergence occurred in **10 sweeps** for both.  \n",
    "- The optimal policy clearly directs the agent **rightward across each row**, then **downward toward the goal (4,4)**.  \n",
    "- Grey cells yield lower values (e.g., \\((0,4)\\), \\((2,2)\\), \\((3,0)\\)) and are avoided in the optimal path.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100b1f92",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c1c2836",
   "metadata": {},
   "source": [
    "## Problem 4 [35]\n",
    "\n",
    "**Off-Policy Monte Carlo with Importance Sampling**  \n",
    "Use the same environment, states, actions, and rewards from Problem 3.  \n",
    "\n",
    "**Task:**  \n",
    "Implement **off-policy Monte Carlo with importance sampling** to estimate the value function for the gridworld.  \n",
    "\n",
    "**Suggested Steps:**  \n",
    "1. Generate multiple episodes using the behavior policy \\(b(a|s)\\).  \n",
    "2. For each episode, calculate the returns (sum of discounted rewards).  \n",
    "3. Use importance sampling to estimate the value function and update the greedy target policy \\(\\pi(a|s)\\).  \n",
    "4. Use \\(\\gamma = 0.9\\).  \n",
    "5. Base your implementation on the main algorithm from Lecture 4.  \n",
    "\n",
    "**Deliverables:**  \n",
    "- Full code with comments.  \n",
    "- Estimated value function for each state.  \n",
    "- Compare results from Monte Carlo vs Value Iteration (optimization time, number of episodes, computational complexity, and observations).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a4c304",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Goal\n",
    "Estimate the optimal **action-value function** \\(Q(s,a)\\) and derive a **greedy policy** \\(\\pi\\) **without using the model** (i.e., no transition probabilities needed). We learn from episodes generated by a **behavior policy** \\(b\\) (uniform random), while evaluating/improving a different **target policy** \\(\\pi\\) (greedy w.r.t. current \\(Q\\)) using **importance sampling**.\n",
    "\n",
    "---\n",
    "\n",
    "### Setup\n",
    "- **Environment:** 5×5 deterministic gridworld from Problem 3  \n",
    "- **Rewards:** goal \\((4,4)=+10\\), grey cells \\((2,2),(3,0),(0,4)=-5\\), others \\(-1\\)  \n",
    "- **Discount:** \\(\\gamma = 0.9\\)  \n",
    "- **Behavior policy \\(b(a|s)\\):** uniform over 4 actions (\\(\\tfrac{1}{4}\\) each)  \n",
    "- **Target policy \\(\\pi(a|s)\\):** greedy w.r.t. current \\(Q\\): \\(\\pi(a|s)=\\mathbb{1}[a=\\arg\\max_{a'} Q(s,a')]\\)\n",
    "\n",
    "---\n",
    "\n",
    "### Why Importance Sampling?\n",
    "We generate data under \\(b\\) but want values **as if** we had followed \\(\\pi\\).  \n",
    "For an episode \\( (S_0,A_0,R_1,\\dots,S_T) \\), the **per-episode importance ratio** up to time \\(t\\) is:\n",
    "\\[\n",
    "\\rho_{t:T-1} \\;=\\; \\prod_{k=t}^{T-1} \\frac{\\pi(A_k|S_k)}{b(A_k|S_k)}.\n",
    "\\]\n",
    "Here, because \\(\\pi\\) is **deterministic greedy**, \\(\\pi(A_k|S_k)\\in\\{0,1\\}\\).  \n",
    "- If \\(A_k\\) is **not** the greedy action, \\(\\pi(A_k|S_k)=0\\Rightarrow \\rho=0\\) → the **backward update stops**.  \n",
    "- If \\(A_k\\) **is** the greedy action, \\(\\pi(A_k|S_k)=1\\) and \\(b(A_k|S_k)=\\tfrac{1}{4}\\), so each matching step multiplies the weight by \\(4\\).\n",
    "\n",
    "---\n",
    "\n",
    "### Algorithm (Weighted Importance Sampling, Control)\n",
    "1. **Collect episodes under \\(b\\):** Start from a fixed start (e.g., \\((0,0)\\)) and sample actions uniformly until terminal (goal) or max steps.\n",
    "2. **Backward pass per episode:**\n",
    "   - Initialize return \\(G \\leftarrow 0\\) and weight \\(W \\leftarrow 1\\).\n",
    "   - For each time step in reverse:\n",
    "     - Update return: \\(G \\leftarrow \\gamma G + R_{t+1}\\).\n",
    "     - If \\(A_t\\) is **not** greedy under current \\(Q\\) at \\(S_t\\), **break** (since \\(\\rho=0\\)).\n",
    "     - Else update using **weighted IS**:\n",
    "       \\[\n",
    "       C(S_t,A_t) \\leftarrow C(S_t,A_t) + W,\\quad\n",
    "       Q(S_t,A_t) \\leftarrow Q(S_t,A_t) + \\frac{W}{C(S_t,A_t)}\\big(G - Q(S_t,A_t)\\big).\n",
    "       \\]\n",
    "     - Update weight: \\(W \\leftarrow W \\cdot \\frac{\\pi(A_t|S_t)}{b(A_t|S_t)} = W \\cdot \\frac{1}{1/4} = 4W\\).\n",
    "3. **Policy improvement:** After updates, re-define \\(\\pi\\) as greedy w.r.t. the new \\(Q\\).\n",
    "4. **Repeat** for many episodes.\n",
    "\n",
    "**Notes**\n",
    "- **Weighted IS** normalizes by cumulative weights \\(C\\), reducing variance vs ordinary IS.\n",
    "- The **break** on non-greedy action is correct because deterministic \\(\\pi\\) gives zero probability to off-greedy actions → ratio collapses to zero.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d89082e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MC Off-Policy Values:\n",
      " [[-0.435225 -1.24     -1.800157  3.091177  4.550572]\n",
      " [ 0.621915  1.793523 -0.913973  4.562701  6.175685]\n",
      " [ 1.793497  3.08845   4.556079  6.18208   7.983318]\n",
      " [ 3.056157  4.545213  6.176953  7.987009 10.      ]\n",
      " [ 4.558875  6.175684  7.987987 10.       10.      ]]\n",
      "MC Off-Policy Policy:\n",
      " [['↓' '↑' '↓' '↓' '↓']\n",
      " ['→' '↓' '↓' '↓' '↓']\n",
      " ['→' '↓' '↓' '↓' '↓']\n",
      " ['→' '→' '↓' '→' '↓']\n",
      " ['→' '→' '→' '→' 'G']]\n"
     ]
    }
   ],
   "source": [
    "# Off-Policy MC (Weighted IS) — compact, corrected, and robust\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "rng = np.random.default_rng(123)                     # reproducibility (change/omit seed if you want)\n",
    "BP = np.full(4, 0.25)                                # behavior probs (uniform over 4 actions)\n",
    "\n",
    "def b_probs(_s): return BP                           # constant behavior distribution\n",
    "def greedy_action(Q, s): return int(np.argmax(Q[s])) if Q[s].size else 0  # greedy wrt Q\n",
    "\n",
    "def generate_episode_b(start=(0,0), max_steps=200, start_random=False):\n",
    "    \"\"\"Episode under behavior b; reward on ARRIVAL; terminal on GOAL.\"\"\"\n",
    "    s = (rng.integers(ROWS), rng.integers(COLS)) if start_random else start\n",
    "    traj = []\n",
    "    for _ in range(max_steps):\n",
    "        if s == GOAL:\n",
    "            traj.append((s, None, 0.0))              # terminal marker\n",
    "            break\n",
    "        a = rng.choice(4, p=BP)                      # sample action from b\n",
    "        s2 = step(s, a)                              # env step (uses Problem 3's step)\n",
    "        r  = R(s2)                                   # reward on ARRIVAL (consistent with VI)\n",
    "        traj.append((s, a, r))\n",
    "        s = s2\n",
    "    return traj\n",
    "\n",
    "def mc_offpolicy_weighted_is(episodes=20000, start=(0,0), gamma=0.9, max_steps=200, start_random=False):\n",
    "    \"\"\"Weighted IS control on Q(s,a); deterministic greedy target π; break on off-greedy action.\"\"\"\n",
    "    Q = defaultdict(lambda: np.zeros(4, dtype=float))   # Q(s,a)\n",
    "    C = defaultdict(lambda: np.zeros(4, dtype=float))   # cumulative weights\n",
    "    for _ in range(episodes):\n",
    "        ep = generate_episode_b(start=start, max_steps=max_steps, start_random=start_random)\n",
    "        G, W = 0.0, 1.0\n",
    "        for (s, a, r) in reversed(ep):\n",
    "            G = gamma * G + r                           # discounted return\n",
    "            if a is None:                               # terminal marker\n",
    "                continue\n",
    "            a_star = greedy_action(Q, s)\n",
    "            if a != a_star:                             # π(a|s)=0 → stop\n",
    "                break\n",
    "            C[s][a] += W\n",
    "            Q[s][a] += (W / C[s][a]) * (G - Q[s][a])    # weighted IS update\n",
    "            W *= 1.0 / BP[a]                            # π=1 (greedy), b=0.25 → ×4\n",
    "    return Q\n",
    "\n",
    "def q_to_V_PI(Q):\n",
    "    \"\"\"Convert Q→(V,π) using Problem 3 globals: ROWS, COLS, GOAL, ARROWS, step, R.\"\"\"\n",
    "    V  = np.zeros((ROWS, COLS), dtype=float)\n",
    "    PI = np.full((ROWS, COLS), '·', dtype=object)\n",
    "    for r in range(ROWS):\n",
    "        for c in range(COLS):\n",
    "            s = (r, c)\n",
    "            if s == GOAL:\n",
    "                V[r, c]  = R(s)                        # +10 at GOAL\n",
    "                PI[r, c] = 'G'\n",
    "                continue\n",
    "            q = Q[s] if Q[s].size else np.zeros(4)\n",
    "            a_star = int(np.argmax(q))\n",
    "            V[r, c]  = q[a_star]\n",
    "            PI[r, c] = ARROWS[a_star]\n",
    "    return V, PI\n",
    "\n",
    "# ---- Run (assumes Problem 3 defined: ROWS, COLS, GOAL, ARROWS, step(), R(), and optionally GAMMA) ----\n",
    "gamma_run = GAMMA if 'GAMMA' in globals() else 0.9\n",
    "Q_mc = mc_offpolicy_weighted_is(episodes=20000, start=(0,0), gamma=gamma_run, max_steps=200, start_random=False)\n",
    "V_mc, PI_mc = q_to_V_PI(Q_mc)\n",
    "\n",
    "print(\"MC Off-Policy Values:\\n\", V_mc)\n",
    "print(\"MC Off-Policy Policy:\\n\", PI_mc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d5655bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MC vs VI -> MAE: 1.9607, Max |diff|: 5.4341\n"
     ]
    }
   ],
   "source": [
    "\n",
    " mae = float(np.mean(np.abs(V_mc - V_sync)))\n",
    " mxd = float(np.max(np.abs(V_mc - V_sync)))\n",
    " print(f\"MC vs VI -> MAE: {mae:.4f}, Max |diff|: {mxd:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427163f0",
   "metadata": {},
   "source": [
    "### Run Settings\n",
    "- Behavior \\(b\\): uniform random (4 actions)  \n",
    "- Target \\(\\pi\\): greedy w.r.t. \\(Q\\) (deterministic)  \n",
    "- \\(\\gamma=0.9\\), **20,000 episodes**, max 200 steps/episode\n",
    "\n",
    "### Brief Comparison vs Value Iteration\n",
    "- **Accuracy:** VI is exact given the model; MC approaches VI with enough episodes (yours shows the right value gradient; a few non-optimal arrows are due to variance).\n",
    "- **Optimization unit:** VI uses **sweeps** (10 here) vs MC **episodes** (20k here).\n",
    "- **Time/complexity:** VI \\(O(|S||A|)\\) per sweep; MC \\(O(\\text{episodes}\\times \\text{avg steps})\\). VI took ~2–3 ms; MC grows with episode count.\n",
    "- **Stability:** VI deterministic (same result every run); MC stochastic (variance ↓ with more episodes).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd671f99",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Strengths & Limitations\n",
    "- **Strengths:**  \n",
    "- Works when the **model is unknown**.  \n",
    "- Can learn directly from **logged data** or exploratory behavior.  \n",
    "- **Limitations:**  \n",
    "- **High variance** from importance sampling.  \n",
    "- Deterministic greedy \\(\\pi\\) causes frequent **breaks**; exploration relies entirely on \\(b\\).  \n",
    "- Needs **large sample sizes** to stabilize.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**Interpretation**\n",
    "- Values increase as states approach the **goal (4,4)** (high numbers in bottom-right).\n",
    "- Policy mostly points **right/down** toward the goal; occasional **←/↑** show **sampling variance** and finite-episode effects.\n",
    "- Slightly negative values near the top/grey areas reflect **step costs** and **penalties**.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "### \n",
    "Off-policy MC with weighted importance sampling learns from **randomly generated episodes** and corrects them toward a **greedy target policy** using importance weights. Your estimates show the right **value gradient** and a mostly **goal-directed policy**, with a few local deviations due to sampling noise. Compared with VI, MC needs **many more samples** but **doesn’t require the model**, making it suitable when transitions are unknown or when learning from logged experience.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fe714e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "813ca0de",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dfc1e5ff",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Brief Comparison: MC Off-Policy vs. Value Iteration \n",
    "\n",
    "- **Accuracy**\n",
    "  - **VI (model-based):** Produces the exact optimal \\(V^*\\) and \\(\\pi^*\\) given the known dynamics; your VI and in-place VI both converged to the same solution in 10 sweeps.\n",
    "  - **MC Off-Policy (model-free):** Approximates \\(V\\) and \\(\\pi\\) from sampled trajectories. Your values are close to VI’s pattern (higher near the goal), with minor deviations and a few non-optimal arrows (e.g., “←”, “↑”) due to sampling variance and finite episodes.\n",
    "\n",
    "- **Data Requirements**\n",
    "  - **VI:** Requires the transition/reward model; few sweeps to converge, low variance.\n",
    "  - **MC Off-Policy:** Requires **many episodes** for stable estimates; no model needed. Increasing episodes typically reduces variance and makes the MC policy align more closely with VI.\n",
    "\n",
    "- **When to Use**\n",
    "  - **Use VI** when the environment model is available and small/moderate in size.\n",
    "  - **Use MC Off-Policy** when the model is unknown but logs or exploratory trajectories from a behavior policy are available.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cd9dc2",
   "metadata": {},
   "source": [
    "## Overall Summary\n",
    "In this assignment, we designed and analyzed reinforcement learning tasks across different settings.  \n",
    "- **Problem 1** modeled a robot pick-and-place task as an MDP, defining states, actions, rewards, and transitions to encourage fast, smooth, and safe behavior.  \n",
    "- **Problem 2** applied value iteration by hand on a 2×2 gridworld to illustrate the Bellman backup process.  \n",
    "- **Problem 3** implemented synchronous and in-place value iteration on a 5×5 gridworld, showing identical optimal values and policies, with comparable convergence performance.  \n",
    "- **Problem 4** applied off-policy Monte Carlo with weighted importance sampling, estimating values and policies without using the model and comparing them with value iteration results.  \n",
    "\n",
    "Together, these problems highlight the trade-off between **model-based methods** (exact, fast when transitions are known) and **model-free methods** (flexible, data-driven, but slower and noisier). This demonstrates the strengths and limitations of different reinforcement learning approaches in practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcffbc4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
